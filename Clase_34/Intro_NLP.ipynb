{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al Procesamiento de Lenguaje Natural (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `El contenido de ésta notebook les va a servir para las clases 33 y 34.`\n",
    "\n",
    "NLP es un subcampo de la informática y la inteligencia artificial relacionadas con las interacciones entre las computadoras y los lenguajes humanos (naturales). Se utiliza para aplicar algoritmos de aprendizaje automático al texto y al habla.\n",
    "\n",
    "Por ejemplo, podemos usar NLP para crear sistemas como reconocimiento de voz, resumen de documentos, traducción automática, detección de correo no deseado, reconocimiento de entidades con nombre, respuesta a preguntas, autocompletado, tipeo predictivo, etc.\n",
    "\n",
    "Hoy en día, la mayoría de nosotros tenemos teléfonos inteligentes que tienen reconocimiento de voz. Estos teléfonos inteligentes usan NLP para entender lo que se dice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a la librería NLTK para Python\n",
    "\n",
    "`NLTK` (Natural Language Toolkit) es una plataforma líder para crear programas de Python para trabajar con datos de lenguaje humano. Proporciona interfaces fáciles de usar para muchos corpus (corpus lingüístico es un conjunto amplio y estructurado de ejemplos reales de uso de la lengua) y recursos léxicos. Además, contiene un conjunto de bibliotecas de procesamiento de texto para clasificación, tokenización, derivación, etiquetado, análisis y razonamiento semántico. Lo mejor de todo es que NLTK es un proyecto gratuito, de código abierto y dirigido por la comunidad.\n",
    "\n",
    "Utilizaremos este kit de herramientas para mostrar algunos conceptos básicos del campo del procesamiento del lenguaje natural.\n",
    "\n",
    "## ¿Cómo instalo `nltk`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://media.giphy.com/media/U1aN4HTfJ2SmgB2BBK/giphy.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La respuesta es simple...\n",
    "\n",
    "<img src='https://i.pinimg.com/236x/5f/57/8f/5f578f04745bb9de96c703738b4b700a.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Los fundamentos de NLP\n",
    "En este notebook, cubriremos los siguientes temas:\n",
    "\n",
    "* Tokenización de oraciones (`Clase 33`)\n",
    "* Tokenización de palabras (`Clase 33`)\n",
    "* Lematización de Texto y Stemming (`Clase 33`)\n",
    "* Stops Words (`Clase 33`)\n",
    "* Regex (`Extra`)\n",
    "* Vectorización: Bag of Words (`Clase 33`)\n",
    "* TF-IDF (`Clase 34`)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenización de oraciones\n",
    "La tokenización de oraciones (también llamada segmentación de oraciones) es el problema de dividir una cadena de lenguaje escrito en sus oraciones componentes. La idea aquí parece muy simple. En muchos idiomas, podemos dividir las oraciones cada vez que vemos un signo de puntuación.\n",
    "\n",
    "Sin embargo, este problema no es trivial debido al uso del carácter de punto final para las abreviaturas. Al procesar texto sin formato, las tablas de abreviaturas que contienen puntos pueden ayudarnos a evitar la asignación incorrecta de los límites de las oraciones. En muchos casos, utilizamos bibliotecas para hacer ese trabajo por nosotros, así que no se preocupen demasiado por los detalles por ahora.\n",
    "\n",
    "### Ejemplo:\n",
    "Veamos un texto sobre un famoso juego de mesa llamado backgammon.\n",
    "\n",
    "`Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.`\n",
    "\n",
    "Para aplicar una tokenización de oración con NLTK podemos usar la función `nltk.sent_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTO ORIGINAL:\n",
      "Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n",
      "\n",
      "ORACIONES:\n",
      "Backgammon is one of the oldest known board games.\n",
      "\n",
      "Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.\n",
      "\n",
      "It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt') #descomentar para descargar datos que usan las funciones de nltk\n",
    "\n",
    "text = \"Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "print(\"TEXTO ORIGINAL:\")\n",
    "print(text)\n",
    "print()\n",
    "\n",
    "print(\"ORACIONES:\")\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenización de palabras\n",
    "\n",
    "La tokenización de palabras (también llamada segmentación de palabras) es el problema de dividir una cadena de lenguaje escrito en sus palabras componentes. En inglés y en muchos otros idiomas que usan alguna forma de alfabeto latino, el espacio es una buena aproximación de un divisor de palabras.\n",
    "\n",
    "Sin embargo, todavía podemos tener problemas si solo dividimos por espacio para lograr los resultados deseados. Algunos sustantivos compuestos en inglés se escriben de forma variable y, a veces, contienen un espacio. En la mayoría de los casos, utilizamos una biblioteca para lograr los resultados deseados, así que nuevamente no se preocupe demasiado por los detalles.\n",
    "\n",
    "### Ejemplo:\n",
    "Usemos las oraciones del paso anterior y veamos cómo podemos aplicar la tokenización de palabras en ellas. Podemos usar la función nltk.word_tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PALABRAS:\n",
      "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.']\n",
      "\n",
      "['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.']\n",
      "\n",
      "['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"PALABRAS:\")\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lematización de Texto y Stemming\n",
    "\n",
    "Por razones gramaticales, los documentos pueden contener diferentes formas de una palabra, como `anda`, `andar`, `andando`. Además, a veces tenemos palabras relacionadas con un significado similar, como `nación`, `nacionalidad`, `nacionalista`.\n",
    "\n",
    "El objetivo del Stemming (derivación) y la lematización es reducir las formas flexivas y, a veces, las formas derivadas de una palabra a una forma básica común.\n",
    "\n",
    "[Fuente](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "### Ejemplos:\n",
    "\n",
    "* `am, are, is` => `be`\n",
    "* `dog, dogs, dog’s, dogs’` => `dog`\n",
    "\n",
    "El resultado de esta asignación aplicada en un texto será algo así:\n",
    "\n",
    "`the boy’s dogs are different sizes => the boy dog be differ size`\n",
    "\n",
    "El stemming y la lematización son casos especiales de `normalización`. Sin embargo, son diferentes entre sí.\n",
    "El stemming generalmente se refiere a un proceso heurístico crudo que corta los extremos de las palabras con la esperanza de lograr cumplir con este objetivo la mayor parte del tiempo, y a menudo incluye la eliminación de afijos derivados.\n",
    "\n",
    "La lematización generalmente se refiere a hacer las cosas correctamente con el uso de un vocabulario y un análisis morfológico de las palabras, normalmente con el objetivo de eliminar solo las terminaciones de inflexión y devolver la forma básica o de diccionario de una palabra, que se conoce como lema.\n",
    "\n",
    "[Fuente](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "La diferencia es que un `stemmer opera sin conocimiento del contexto` y, por lo tanto, no puede entender la diferencia entre palabras que tienen un significado diferente dependiendo de una parte del discurso. Pero los stemmers también tienen algunas ventajas, son más fáciles de implementar y generalmente funcionan más rápido. Además, la \"precisión\" reducida puede no ser importante para algunas aplicaciones.\n",
    "\n",
    "### Ejemplos:\n",
    "\n",
    "La palabra `better` tiene `good` como lema. Este enlace se pierde al derivar, ya que requiere una búsqueda en el diccionario.\n",
    "\n",
    "La palabra `play` es la forma básica de la palabra `playing` y, por lo tanto, esto se corresponde tanto con la derivación como con la lematización.\n",
    "\n",
    "La palabra `meeting` puede ser la forma básica de un sustantivo o la forma de un verbo (`to meet`) según el contexto; por ejemplo, `in our last meeting` o `We are meeting again tomorrow`. A diferencia de la derivación, la lematización intenta seleccionar el lema correcto según el contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: seen\n",
      "Stemmer: seen\n",
      "Lemmatizer: see\n",
      "\n",
      "Original: drove\n",
      "Stemmer: drove\n",
      "Lemmatizer: drive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('wordnet') #descomentar para descargar datos que usan las funciones de nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):\n",
    "    \"\"\"\n",
    "    Print the results of stemmind and lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)\n",
    "    \"\"\"\n",
    "    print(\"Stemmer:\", stemmer.stem(word))\n",
    "    print(\"Lemmatizer:\", lemmatizer.lemmatize(word, pos))\n",
    "    print()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "print(\"Original: seen\")\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"seen\", pos = wordnet.VERB)\n",
    "\n",
    "print(\"Original: drove\")\n",
    "compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"drove\", pos = wordnet.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stop Words\n",
    "\n",
    "Las palabras de detención son palabras que se filtran antes o después del procesamiento del texto. Al aplicar el aprendizaje automático al texto, estas palabras pueden agregar mucho ruido. Por eso queremos eliminar estas palabras irrelevantes.\n",
    "\n",
    "Las palabras de detención generalmente se refieren a las palabras más comunes como “y”, “la”, “a” en un idioma, pero no existe una lista universal única de palabras de detención. La lista de palabras de detención puede cambiar según su aplicación.\n",
    "\n",
    "La herramienta NLTK tiene una lista predefinida de palabras vacías que se refieren a las palabras más comunes. Si lo usa por primera vez, debe descargar las palabras de detención. Una vez que completamos la descarga, podemos cargar el paquete de palabras de parada de nltk.corpus y usarlo para cargar las palabras de parada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download(\"stopwords\") #descomentar para descargar datos que usan las funciones de nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo podemos eliminar las palabras vacías de una oración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "sentence = \"Backgammon is one of the oldest known board games.\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "without_stop_words = [word for word in words if not word in stop_words]\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si todavía no estás familiarizado con la lista de comprensiones en Python. Aquí hay otra forma de lograr el mismo resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "sentence = \"Backgammon is one of the oldest known board games.\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "without_stop_words = []\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        without_stop_words.append(word)\n",
    "\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regex\n",
    "\n",
    "Una expresión regular, regex o regexp es una secuencia de caracteres que definen un `patrón de búsqueda`. Veamos algunos conceptos básicos.\n",
    "\n",
    "* `\\t` — Representa un tabulador.\n",
    "* `\\r` — Representa el \"retorno de carro\" o \"regreso al inicio\" o sea el lugar en que la línea vuelve a iniciar.\n",
    "* `\\n` — Representa la \"nueva línea\" el carácter por medio del cual una línea da inicio. Es necesario recordar que en Windows es necesaria una combinación de \\r\\n para comenzar una nueva línea, mientras que en Unix solamente se usa \\n y en Mac_OS clásico se usa solamente \\r.\n",
    "* `\\a` — Representa una \"campana\" o \"beep\" que se produce al imprimir este carácter.\n",
    "* `\\e` — Representa la tecla \"Esc\" o \"Escape\"\n",
    "* `\\f` — Representa un salto de página\n",
    "* `\\v` — Representa un tabulador vertical\n",
    "* `\\x` — Se utiliza para representar caracteres ASCII o ANSI si conoce su código. De esta forma, si se busca el símbolo de derechos de autor y la fuente en la que se busca utiliza el conjunto de caracteres latín-1 es posible encontrarlo utilizando \\xA9\".\n",
    "* `\\u` — Se utiliza para representar caracteres Unicode si se conoce su código. \"\\u00A2\" representa el símbolo de centavos. No todos los motores de Expresiones Regulares soportan Unicode. El .Net Framework lo hace, pero el EditPad Pro no, por ejemplo.\n",
    "* `\\d` — Representa un dígito del 0 al 9.\n",
    "* `\\w` — Representa cualquier carácter alfanumérico.\n",
    "* `\\s` — Representa un espacio en blanco.\n",
    "* `\\D` — Representa cualquier carácter que no sea un dígito del 0 al 9.\n",
    "* `\\W` — Representa cualquier carácter no alfanumérico.\n",
    "* `\\S` — Representa cualquier carácter que no sea un espacio en blanco.\n",
    "* `\\A` — Representa el inicio de la cadena. No un carácter sino una posición.\n",
    "* `\\Z` — Representa el final de la cadena. No un carácter sino una posición.\n",
    "* `\\b` — Marca la posición de una palabra limitada por espacios en blanco, puntuación o el inicio/final de una cadena.\n",
    "* `\\B` — Marca la posición entre dos caracteres alfanuméricos o dos no-alfanuméricos.\n",
    "* `[abc]` coincide con cualquiera de a, b o c\n",
    "* `[^abc]` no coincide con a, b o c\n",
    "* `[a-g]` coincide con un caracter entre a & g\n",
    "\n",
    "[Fuente](https://docs.python.org/3/library/re.html?highlight=regex)\n",
    "\n",
    "Podemos usar expresiones regulares para aplicar filtros adicionales a nuestro texto. Por ejemplo, podemos eliminar todos los caracteres que no son palabras. En muchos casos, no necesitamos los signos de puntuación y es fácil eliminarlos con expresiones regulares.\n",
    "\n",
    "En Python, el módulo `re` proporciona operaciones de coincidencia de expresiones regulares similares a las de Perl. Podemos usar la función `re.sub` para reemplazar las coincidencias de un patrón con una cadena de reemplazo. Veamos un ejemplo cuando reemplazamos todas las no palabras con el carácter de espacio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The development of snowboarding was inspired by skateboarding  sledding  surfing and skiing \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = \"The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing.\"\n",
    "pattern = r\"[^\\w]\" \n",
    "print(re.sub(pattern, \" \", sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vectorización: Bag of Words\n",
    "\n",
    "Los algoritmos de aprendizaje automático no pueden funcionar con texto sin formato directamente, necesitamos convertir el `texto en números`. Esto se llama extracción de características.\n",
    "\n",
    "El modelo de `bolsa de palabras` es una técnica de extracción de características popular y simple que se utiliza cuando trabajamos con texto. Describe la aparición de cada palabra dentro de un documento.\n",
    "\n",
    "Para usar este modelo, necesitamos:\n",
    "* Diseñar un vocabulario de palabras conocidas (también llamadas `tokens`)\n",
    "* Elegir una medida de la presencia de palabras conocidas\n",
    "\n",
    "Cualquier información sobre el orden o la estructura de las palabras se descarta, por eso se llama bolsa de palabras. Este modelo intenta comprender si una palabra conocida aparece en un documento, pero no sabe dónde está esa palabra en el documento.\n",
    "\n",
    "La intuición es que documentos similares tienen contenidos similares. Además, de un contenido, podemos aprender algo sobre el significado del documento.\n",
    "\n",
    "### Ejemplo:\n",
    "Veamos cuáles son los pasos para crear un modelo de bolsa de palabras. En este ejemplo, usaremos solo cuatro oraciones para ver cómo funciona este modelo. En los problemas del mundo real, trabajará con cantidades de datos mucho mayores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `A. Cargar los datos`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digamos que estos son nuestros datos y queremos cargarlos como una matriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`I like this movie, it's funny.\n",
    "I hate this movie.\n",
    "This was awesome! I like it.\n",
    "Nice one. I love it.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para lograr esto, simplemente podemos leer el archivo y dividirlo por líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I like this movie, it's funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"review.txt\", \"r\") as file:\n",
    "    documents = file.read().splitlines()\n",
    "    \n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `B. Diseño del vocabulario`\n",
    "\n",
    "Consigamos todas las palabras únicas de las cuatro oraciones cargadas ignorando la puntuación y los tokens de un carácter (por ejemplo: `,`, `.`, `:`, etc). Estas palabras serán nuestro __vocabulario__ (palabras conocidas).\n",
    "\n",
    "Podemos usar la clase [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de la biblioteca `sklearn` para diseñar nuestro vocabulario. Veremos también cómo podemos usarlo después de leer el siguiente paso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `C. Crear los vectores del documento`\n",
    "\n",
    "Luego, debemos puntuar las palabras en cada documento. La tarea aquí es convertir cada texto sin formato en un vector de números. Después de eso, podemos usar estos vectores como entrada para un modelo de aprendizaje automático. El método de puntuación más simple es marcar la presencia de palabras con 1 para presente y 0 para ausente.\n",
    "\n",
    "Ahora, veamos cómo podemos crear un modelo de bolsa de palabras utilizando la clase CountVectorizer mencionada anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   awesome  funny  hate  it  like  love  movie  nice  one  this  was\n",
       "0        0      1     0   1     1     0      1     0    0     1    0\n",
       "1        0      0     1   0     0     0      1     0    0     1    0\n",
       "2        1      0     0   1     1     0      0     0    0     1    1\n",
       "3        0      0     0   1     0     1      0     1    1     0    0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the libraries we need\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Design the Vocabulary\n",
    "# The default token pattern removes tokens of a single character. \n",
    "# That's why we don't have the \"I\" and \"s\" tokens in the output\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 3. Create the Bag-of-Words Model\n",
    "bag_of_words = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Show the Bag-of-Words Model as a pandas DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genial! Pudimos convertir esto:\n",
    "    \n",
    "`I like this movie, it's funny.\n",
    "I hate this movie.\n",
    "This was awesome! I like it.\n",
    "Nice one. I love it.`\n",
    "\n",
    "En números! De ahora en más podemos usar estos datos vectorizados con cualquiera de los modelos de machine learning que hemos visto.\n",
    "\n",
    "Un buen ejemplo de este tipo de problemas, es el conocido como `Análisis de Sentimientos`. En términos generales, el análisis de sentimiento intenta determinar la actitud de un interlocutor o usuario con respecto a algún tema o la polaridad contextual general de un documento. La actitud puede ser su juicio o evaluación, estado afectivo (o sea, el estado emocional del autor al momento de escribir), o la intención comunicativa emocional (o sea, el efecto emocional que el autor intenta causar en el lector).\n",
    "\n",
    "[Fuente](https://es.wikipedia.org/wiki/An%C3%A1lisis_de_sentimiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.generadormemes.com/media/created/xuvogogh0j87fws3ova74zy83muw78en2ixbiep402454ssglichbj2p2kbzjfkg.jpg.pagespeed.ic.imagenes-memes-fotos-frases-graciosas-chistosas-divertidas-risa-chida-espa%C3%B1ol-whatsapp-facebook.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. TF-IDF (Clase 34)\n",
    "\n",
    "Un problema con la frecuencia de puntuación de palabras es que las palabras más frecuentes en el documento comienzan a tener las puntuaciones más altas. Estas palabras frecuentes pueden no contener tanta \"ganancia informativa\" para el modelo en comparación con algunas palabras más raras y específicas del dominio. Un enfoque para solucionar ese problema es penalizar las palabras que son frecuentes en todos los documentos. Este enfoque se llama `TF-IDF.`\n",
    "\n",
    "TF-IDF, abreviatura de `Term Frequency-Inverse Document Frequency` es una medida estadística utilizada para evaluar la importancia de una palabra para un documento en una colección o corpus.\n",
    "\n",
    "El valor de puntuación TF-IDF aumenta proporcionalmente al número de veces que aparece una palabra en el documento, pero está compensado por el número de documentos en el corpus que contienen la palabra.\n",
    "\n",
    "Veamos la fórmula utilizada para calcular un puntaje TF-IDF para un término dado `x` dentro de un documento `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/a.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, dividamos un poco esta fórmula y veamos cómo funcionan las diferentes partes.\n",
    "\n",
    "`Término Frecuencia (TF):` una puntuación de la frecuencia de la palabra en el documento actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/b.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Frecuencia de término inverso (ITF):` una puntuación de cuán rara es la palabra en los documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/c.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos usar las fórmulas anteriores para calcular el puntaje `TF-IDF` para un término dado como este:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/d.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo:\n",
    "\n",
    "En Python, podemos usar la clase `TfidfVectorizer` de la biblioteca `sklearn` para calcular los puntajes de TF-IDF para documentos dados. Usemos las mismas oraciones que hemos usado con el ejemplo de la bolsa de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365003</td>\n",
       "      <td>0.450852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.539445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.425305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.539445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    awesome     funny      hate        it      like      love     movie  \\\n",
       "0  0.000000  0.571848  0.000000  0.365003  0.450852  0.000000  0.450852   \n",
       "1  0.000000  0.000000  0.702035  0.000000  0.000000  0.000000  0.553492   \n",
       "2  0.539445  0.000000  0.000000  0.344321  0.425305  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.345783  0.000000  0.541736  0.000000   \n",
       "\n",
       "       nice       one      this       was  \n",
       "0  0.000000  0.000000  0.365003  0.000000  \n",
       "1  0.000000  0.000000  0.448100  0.000000  \n",
       "2  0.000000  0.000000  0.344321  0.539445  \n",
       "3  0.541736  0.541736  0.000000  0.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resumen\n",
    "\n",
    "* NLP se utiliza para aplicar algoritmos de aprendizaje automático a texto y voz.\n",
    "* NLTK (Natural Language Toolkit) es una plataforma líder para crear programas de Python para trabajar con datos de lenguaje humano.\n",
    "* La tokenización de oraciones es el problema de dividir una cadena de lenguaje escrito en sus oraciones componentes.\n",
    "* La tokenización de palabras es el problema de dividir una cadena de lenguaje escrito en sus palabras componentes.\n",
    "* El objetivo de la derivación y la lematización es reducir las formas flexivas y, a veces, las formas derivadas de una palabra a una forma básica común.\n",
    "* Las palabras de detención son palabras que se filtran antes o después del procesamiento del texto. Por lo general, se refieren a las palabras más comunes en un idioma.\n",
    "* Una expresión regular es una secuencia de caracteres que definen un patrón de búsqueda.\n",
    "* El modelo de bolsa de palabras es una técnica de extracción de características popular y simple que se utiliza cuando trabajamos con texto. Describe la aparición de cada palabra dentro de un documento.\n",
    "* TF-IDF es una medida estadística utilizada para evaluar la importancia de una palabra para un documento en una colección o corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
